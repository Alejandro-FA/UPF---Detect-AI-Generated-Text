import shap
import streamlit as st
import streamlit.components.v1 as components
from transformers import pipeline, TextClassificationPipeline
from utils import get_torch_device
from dataclasses import dataclass
import os
import matplotlib.pyplot as plt


@dataclass(repr=False, eq=False, frozen=True)
class Model:
    name: str
    pipeline: TextClassificationPipeline
    explainer: shap.Explainer


@st.cache_resource
def load_models() -> list[Model]:
    device = get_torch_device(debug=True)
    our_model = pipeline(
        task='text-classification',
        model=os.path.abspath('model'),
        device=device,
        top_k=1,
        truncation=True,
        padding=True,
    )
    simpleai_model = pipeline(
        task="text-classification",
        model="Hello-SimpleAI/chatgpt-detector-roberta",
        device=device,
        top_k=1,
        truncation=True,
        padding=True,
    )
    return [
        Model('ours', our_model, shap.Explainer(our_model)),
        Model('simpleai', simpleai_model, shap.Explainer(simpleai_model)),
    ]


@st.cache_data(
    show_spinner="Running model to check if this text has been generated by an AI...",
    hash_funcs={Model: lambda x: hash(x.name)}
)
def _predict_aux(text: str, model: Model) -> None:
    st.session_state[f'text_input_{model.name}'] = text
    st.session_state[f'prediction_{model.name}'] = model.pipeline(text)
    st.session_state[f'shap_values_{model.name}'] = model.explainer([text])


def compute_prediction(text: str, model: Model) -> None:
    _predict_aux(text, model)
    st.toast('Prediction completed!', icon="âœ…")


def init_state(model_name: str) -> None:
    # Initialize session state
    if f'prediction_{model_name}' not in st.session_state:
        st.session_state[f'prediction_{model_name}'] = None
    if f'shap_values_{model_name}' not in st.session_state:
        st.session_state[f'shap_values_{model_name}'] = None
    if f'text_input_{model_name}' not in st.session_state:
        st.session_state[f'text_input_{model_name}'] = None


def input_text_widget(model: Model) -> str:
    return st.text_area(
        label="Enter text to check",
        value=st.session_state[f'text_input_{model.name}'],
        max_chars=2500,
        height=300,
        placeholder="Copy the text that you want to check here.",
        label_visibility="collapsed",
        on_change=lambda: compute_prediction(
            st.session_state[f'text_area_{model.name}'],
            model
        ),
        key=f'text_area_{model.name}',
    )


def run_model_button(model_name: str) -> bool:
    return st.button(
        'Check text',
        type='primary',
        use_container_width=True,
        key=f'button_{model_name}',
    )
        

def plot_shap(model_name: str, labels={0:'human', 1:'ai'}) -> None:
    prediction = st.session_state[f'prediction_{model_name}']
    shap_values = st.session_state[f'shap_values_{model_name}']

    if prediction and shap_values:
        # Display prediction result
        prob = prediction[0][0]['score']
        if prediction[0][0]['label'] == labels[0]: prob = 1 - prob
        st.subheader(f'There is a :blue[{round(prob * 100, 2)}%] chance that this text has been generated by an AI model.')

        # Explainable AI plots
        html = shap.plots.text(shap_values[:, :, prediction[0][0]['label']], display=False)
        height = 160 + 20*len(st.session_state[f'text_input_{model_name}']) / 100
        components.html(html, scrolling=True, height=int(height))

        st.write('\n')

        shap.plots.bar(shap_values[0, :, prediction[0][0]['label']], show=False)
        fig = plt.gcf()
        st.pyplot(fig)
        plt.cla()
        plt.close(fig)


if __name__ == '__main__':
    st.set_page_config(
        page_title="AI-generated text detection",
        page_icon="ðŸ¤–",
        layout="centered",
        initial_sidebar_state="auto",
    )

    ############################ SECTION 00 ###################################
    st.title("Check whether a text has been generated by an AI or not ðŸ¤–")
    st.write('\n')
    st.markdown(
        """
        You can make a prediction with either our own model or with one of the
        2 models developed by [SimpleAI](https://github.com/Hello-SimpleAI/chatgpt-comparison-detection)Â¹. If you want more information about our
        model, we recommend to check the [training dataset analysis](/Corpus_analysis)
        and the [model performance](/Model_evaluation).
        """
    )
    st.caption(
        """
        Â¹Guo, B., Zhang, X., Wang, Z., Jiang, M., Nie, J., Ding, Y., ... & Wu,
        Y. (2023). How close is chatgpt to human experts? comparison corpus,
        evaluation, and detection. arXiv preprint arXiv:2301.07597
        """
    )
    
    ############################ SECTION 01 ###################################
    # Load AI-generatet text classification model and SHAP explainers
    models = load_models()
    tab1, tab2 = st.tabs(["Our model", "SimpleAI ChatGPT Detector"])

    with tab1:
        init_state(models[0].name)
        txt = input_text_widget(models[0])
    
        st.write('\n')
        col1, col2, col3 = st.columns([0.4, 0.2, 0.4])
        with col2:
            if run_model_button(models[0].name):
                compute_prediction(txt, models[0])

        st.write('\n')
        st.write('\n')
        plot_shap(models[0].name, labels={0:'human', 1:'ai'})

    with tab2:
        init_state(models[1].name)
        txt = input_text_widget(models[1])
    
        st.write('\n')
        col1, col2, col3 = st.columns([0.4, 0.2, 0.4])
        with col2:
            if run_model_button(models[1].name):
                compute_prediction(txt, models[1])

        st.write('\n')
        st.write('\n')
        plot_shap(models[1].name, labels={0:'Human', 1:'ChatGPT'})
        
