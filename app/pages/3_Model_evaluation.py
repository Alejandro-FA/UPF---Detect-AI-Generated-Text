import streamlit as st
from utils import plot_confusion_matrix
import numpy as np
from sklearn.metrics import (
    accuracy_score, 
    confusion_matrix,
    classification_report,
)

primary_color = 'red'
secondary_color = 'blue'


def as_perc(x: float) -> float:
    return str(round(x * 100, 1)) + " %"


def _precision_recall_f1(values: dict[str, float], values_comp: dict[str, float], label: str) -> None:
    st.metric(
        label=f':{secondary_color}[Precision]',
        value=as_perc(values['precision']),
        help=f'Proportion of texts that have been classified as "{label}-generated" that have been actually generated by a {label}',
        delta=as_perc(values['precision'] - values_comp['precision']),
    )

    st.metric(
        label=f':{secondary_color}[Recall]',
        value=as_perc(values['recall']),
        help=f'Proportion of {label}-generated texts that have been correctly classified by the model',
        delta=as_perc(values['recall'] - values_comp['recall']),
    )

    st.metric(
        label=f':{secondary_color}[F1-score]',
        value=round(values['f1-score'], 3),
        help='F1-score can range from 0 (if either precision or recall are 0) to 1 (indicating perfect precision and recall)',
        delta=round(values['f1-score'] - values_comp['f1-score'], 3),
    )


def metrics_tab(y_true: np.ndarray, y_pred: np.ndarray, y_true_other: np.ndarray, y_pred_other: np.ndarray):
    col1, col2 = st.columns(2)

    label2id = {"Human": 0, "AI-generated": 1}
    report = classification_report(y_true, y_pred, target_names=label2id.keys(), digits=4, output_dict=True)
    report_other = classification_report(y_true_other, y_pred_other, target_names=label2id.keys(), digits=4, output_dict=True)

    with col1:
        accuracy = accuracy_score(y_true, y_pred)
        accuracy_other = accuracy_score(y_true_other, y_pred_other)
        st.metric(
            label=f':{secondary_color}[Accuracy]',
            value=as_perc(accuracy),
            help='Proportion of texts that have been correctly classified',
            delta=as_perc(accuracy - accuracy_other),
        )

        col3, col4 = st.columns(2)
        with col3:
            st.markdown(f'#### Human-generated texts')
            _precision_recall_f1(report['Human'], report_other['Human'], 'Human')

        with col4:
            st.markdown(f'#### AI-generated texts')
            _precision_recall_f1(report['AI-generated'], report_other['AI-generated'], 'AI')

    with col2:
        st.markdown(f':{secondary_color}[Confusion matrix]')
        cm = confusion_matrix(y_true, y_pred, normalize='true')
        st.pyplot( plot_confusion_matrix(cm, classes=label2id.keys(), figsize=(8, 6), is_norm=True) )


if __name__ == '__main__':
    import pandas as pd

    st.set_page_config(
        page_title="Model evaluation",
        page_icon="ü©∫",
        layout="wide",
        initial_sidebar_state="auto",
    )

    DATA_FOLDER = 'data'
    MODEL_FOLDER = 'model'

    ############################ SECTION 00 ###################################
    st.sidebar.markdown(
        """Here you can check how reliable are the models at detecting AI-generated
        texts. We also provide some information about the test methodology.
        """
    )
    st.title('Performance evaluation of classification models ü©∫')
    st.write('\n')

    with st.expander(label=f':{primary_color}[Evaluation method]', expanded=False):
        col1, col2 = st.columns(2)

        with col1:
            st.subheader(f':{primary_color}[Test dataset information]')
            body = """
            A **test dataset** should only contain data that has never been seen
            by the evaluated model. Furthermore, it should also be representative
            of the goal task that the model is designed to accomplish.

            In order to achieve this, we have built a semi-custom dataset from
            2 recently published corpus: [CHEAT (Yu et al. 2023)](https://arxiv.org/abs/2304.12008)
            and [MGTBench (He et al. 2023)](https://arxiv.org/abs/2303.14822).
            Here we provide some relevant details:

            - The CHEAT corpus is compromised by human-generated and ChatGPT-generated
            scientific abstracts. We have randomly selected `750` records from this
            corpus.

            - The MGTBench benchmark uses 3 different corpus for evaluating
            AI-generated text classification models. From these 3 corpus we have
            only used *TruthfulQA*, since it ressembles our use case the most.
            We have selected all the `793` non-null records from this dataset.

            > **NOTE** our model has been trained with [this dataset](https://www.kaggle.com/datasets/jdragonxherrera/augmented-data-for-llm-detect-ai-generated-text/data), while the
            [SimpleAI ChatGPT Detector (Guo et al. 2023)](https://arxiv.org/abs/2301.07597)
            model uses their own HC3 dataset.
            """

            st.markdown(body)

        with col2:
            st.subheader(f':{primary_color}[Evaluation metrics and interpretation]')

            body = f"""
            For evaluating the performance of the models we have used several
            metrics. Here you can find a brief explanation on how to interpret
            these metrics.

            #### :{secondary_color}[Accuracy]
            The accuracy is the **proportion of texts that have been correctly
            classified**. Higher values are better. It is the least informative
            metric.

            #### :{secondary_color}[Precision]
            The precision is the **fraction of true positives** ("human-generated"
            texts or "AI-generated" texts classified correctly by the model)
            **among all classified possitives** (texts that have been classified
            by the model as "human-generated" or "AI-generated" including errors).
            Higher is better.

            #### :{secondary_color}[Recall]
            The recall is the **fraction of true positives** ("human-generated"
            texts or "AI-generated" texts classified correctly by the model)
            **among all actual possitives** (all texts that actually are
            "human-generated" or "AI-generated"). Higher is better.

            #### :{secondary_color}[F1 score]
            The f1-score is a simple metric for evaluating binary classification
            models that addresses some of the issues of the accuracy. It is
            the harmonic mean of the `precision` and the `recall`:"""
            st.markdown(body)
            st.latex(r'F_1 = 2\cdot\frac{precision \cdot recall}{precision + recall}')

            body = f"""
            Its value can go from 0 (if either precision or recall are zero) to
            1 (indicating perfect precision and recall).
            """

            st.markdown(body)

    ############################ SECTION 01 ###################################
    with st.expander(label=f':{primary_color}[Test dataset]', expanded=False):
        df = pd.read_csv(f'{DATA_FOLDER}/test_dataset.csv')
        st.dataframe(df, use_container_width=True, column_config={
            'text': st.column_config.TextColumn(
                label='Text ‚úèÔ∏è',
                help='The text to determine whether it has been generated by a human of by a LLM',
                # max_chars=250,
                # width='large',
            ),
            'label': st.column_config.NumberColumn(
                label='AI-generated ü§ñ',
                help='Whether this text has been generate by a LLM or not',
                width='small',
            ),
            'source': st.column_config.SelectboxColumn(
                label='Source üìç',
                help='Original corpus',
                width='small',
            )
        })

    ############################ SECTION 02 ###################################
    st.header('Performance metrics comparison üìä')
    st.markdown('Compare how good is each model at detecting AI-generated texts. The deltas shown are between the models.')
    tab1, tab2 = st.tabs(["Our model", "SimpleAI ChatGPT Detector"])

    y_true_ours = np.load(f'{DATA_FOLDER}/y_true_ours.npy')
    y_pred_ours = np.load(f'{DATA_FOLDER}/y_pred_ours.npy')
    y_true_roberta = np.load(f'{DATA_FOLDER}/y_true_roberta.npy')
    y_pred_roberta = np.load(f'{DATA_FOLDER}/y_pred_roberta.npy')
    with tab1:
        metrics_tab(y_true_ours, y_pred_ours, y_true_roberta, y_pred_roberta)

    with tab2:
        metrics_tab(y_true_roberta, y_pred_roberta, y_true_ours, y_pred_ours)
    